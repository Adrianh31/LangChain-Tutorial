{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d032cf26",
   "metadata": {},
   "source": [
    "# Building Question Answering Application using OpenAI, Pinecone, and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796aa28d",
   "metadata": {},
   "source": [
    "LLM models are great at answering questions but only on topics they have been trained on. However, it wouldn't understand our question if we want it to answer questions about topics it hasn't been trained on, such as recent events after September 2021. In this notebook we are going to build an application that will allow us to send questions to our document and private data and get answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02521d",
   "metadata": {},
   "source": [
    "# Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345e36aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/ds/anaconda3/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/ds/anaconda3/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from openai) (0.24.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/ds/anaconda3/lib/python3.10/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/ds/anaconda3/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from openai) (1.10.12)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ds/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/ds/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (0.17.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ds/anaconda3/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: langchain in /Users/ds/anaconda3/lib/python3.10/site-packages (0.1.0)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (0.0.12)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (0.0.80)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (0.5.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (0.1.10)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ds/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ds/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ds/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/ds/anaconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ds/anaconda3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/ds/anaconda3/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.7->langchain) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ds/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ds/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ds/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ds/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: pinecone-client in /Users/ds/anaconda3/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pinecone-client) (0.7.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pinecone-client) (4.65.0)\n",
      "Requirement already satisfied: numpy in /Users/ds/anaconda3/lib/python3.10/site-packages (from pinecone-client) (1.25.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pinecone-client) (2.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pinecone-client) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pinecone-client) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pinecone-client) (1.26.16)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ds/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ds/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pinecone-client) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ds/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pinecone-client) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ds/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "Requirement already satisfied: python-dotenv in /Users/ds/anaconda3/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: tiktoken in /Users/ds/anaconda3/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ds/anaconda3/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ds/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ds/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ds/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ds/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install pinecone-client\n",
    "!pip install python-dotenv\n",
    "!pip install tiktoken\n",
    "!pip install pypdf -q\n",
    "!pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c931b8e1",
   "metadata": {},
   "source": [
    "### Loading Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35c3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "print(\"API Key Loaded:\", os.environ.get('OPENAI_API_KEY') is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b854dcc7",
   "metadata": {},
   "source": [
    "### Function loading documents with different formats (pdf, docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "256cd511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "    \n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "    \n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e9a1c",
   "metadata": {},
   "source": [
    "### Testing the function using PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca0ac034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Files/GNNs.pdf\n",
      "You have 25 pages in your data\n",
      "There are 6419 characters in the page\n",
      "Graph neural networks: A review of methods and applications\n",
      "Jie Zhoua,1, Ganqu Cuia,1, Shengding Hua, Zhengyan Zhanga, Cheng Yangb, Zhiyuan Liua,*,\n",
      "Lifeng Wangc, Changcheng Lic, Maosong Suna\n",
      "aDepartment of Computer Science and Technology, Tsinghua University, Beijing, China\n",
      "bSchool of Computer Science, Beijing University of Posts and Telecommunications, China\n",
      "cTencent Incorporation, Shenzhen, China\n",
      "ARTICLE INFO\n",
      "Keywords:\n",
      "Deep learningGraph neural networkABSTRACT\n",
      "Lots of learning tasks require dealing with graph data which contains rich relation information among elements.\n",
      "Modeling physics systems, learning molecular ﬁngerprints, predicting protein interface, and classifying diseases\n",
      "demand a model to learn from graph inputs. In other domains such as learning from non-structural data like textsand images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of\n",
      "images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs)\n",
      "are neural models that capture the dependence of graphs via message passing between the nodes of graphs. Inrecent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph\n",
      "recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this\n",
      "survey, we propose a general design pipeline for GNN models and discuss the variants of each component, sys-tematically categorize the applications, and propose four open problems for future research.\n",
      "1. Introduction\n",
      "Graphs are a kind of data structure which models a set of objects\n",
      "(nodes) and their relationships (edges). Recently, researches on\n",
      "analyzing graphs with machine learning have been receiving more and\n",
      "more attention because of the great expressive power of graphs, i.e.\n",
      "graphs can be used as denotation of a large number of systems across\n",
      "various areas including social science (social networks ( Wu et al., 2020 ),\n",
      "natural science (physical systems ( Sanchez et al., 2018 ;Battaglia et al.,\n",
      "2016 ) and protein-protein interaction networks ( Fout et al., 2017 )),\n",
      "knowledge graphs ( Hamaguchi et al., 2017 ) and many other research\n",
      "areas ( Khalil et al., 2017 ). As a unique non-Euclidean data structure for\n",
      "machine learning, graph analysis focuses on tasks such as node classi ﬁ-\n",
      "cation, link prediction, and clustering. Graph neural networks (GNNs) are\n",
      "deep learning based methods that operate on graph domain. Due to its\n",
      "convincing performance, GNN has become a widely applied graph\n",
      "analysis method recently. In the following paragraphs, we will illustrate\n",
      "the fundamental motivations of graph neural networks.\n",
      "Theﬁrst motivation of GNNs roots in the long-standing history ofneural networks for graphs. In the nineties, Recursive Neural Networks\n",
      "areﬁrst utilized on directed acyclic graphs ( Sperduti and Starita, 1997 ;\n",
      "Frasconi et al., 1998 ). Afterwards, Recurrent Neural Networks and\n",
      "Feedforward Neural Networks are introduced into this literature\n",
      "respectively in ( Scarselli et al., 2009 ) and ( Micheli, 2009 ) to tackle cy-\n",
      "cles. Although being successful, the universal idea behind these methods\n",
      "is building state transition systems on graphs and iterate until conver-\n",
      "gence, which constrained the extendability and representation ability.\n",
      "Recent advancement of deep neural networks, especially convolutional\n",
      "neural networks (CNNs) ( LeCun et al., 1998 ) result in the rediscovery of\n",
      "GNNs. CNNs have the ability to extract multi-scale localized spatial\n",
      "features and compose them to construct highly expressive representa-\n",
      "tions, which led to breakthroughs in almost all machine learning areas\n",
      "and started the new era of deep learning ( LeCun et al., 2015 ). The keys of\n",
      "CNNs are local connection, shared weights and the use of multiple layers\n",
      "(LeCun et al., 2015 ). These are also of great importance in solving\n",
      "problems on graphs. However, CNNs can only operate on regular\n",
      "Euclidean data like images (2D grids) and texts (1D sequences) while\n",
      "these data structures can be regarded as instances of graphs. Therefore, it\n",
      "* Corresponding author.\n",
      "E-mail addresses: zhoujie18@mails.tsinghua.edu.cn (J. Zhou), cgq19@mails.tsinghua.edu.cn (G. Cui), hsd20@mails.tsinghua.edu.cn (S. Hu), zy-z19@mails.\n",
      "tsinghua.edu.cn (Z. Zhang), albertyang33@gmail.com (C. Yang), liuzy@tsinghua.edu.cn (Z. Liu), fandywang@tencent.com (L. Wang), harrychli@tencent.com\n",
      "(C. Li), sms@tsinghua.edu.cn (M. Sun).\n",
      "1indicates equal contribution.\n",
      "Contents lists available at ScienceDirect\n",
      "AI Open\n",
      "journal homepage: www.keaipublishing.com/en/journals/ai-open\n",
      "https://doi.org/10.1016/j.aiopen.2021.01.001\n",
      "Received 16 September 2020; Received in revised form 15 December 2020; Accepted 27 January 2021\n",
      "Available online 8 April 20212666-6510/ ©2021 The Author(s). Published by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY license\n",
      "(http://creativecommons.org/licenses/by/4.0/ ).AI Open 1 (2020) 57 –81\n",
      "{'source': 'Files/GNNs.pdf', 'page': 5}\n"
     ]
    }
   ],
   "source": [
    "data = extract_text_from_document('Files/GNNs.pdf')\n",
    "print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[5].page_content)} characters in the page')\n",
    "print(data[0].page_content)\n",
    "print(data[5].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9e4d4",
   "metadata": {},
   "source": [
    "### Testing the function using word file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d6ec0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Files/Summary.docx\n",
      "You have 1 pages in your data\n",
      "There are 2153 characters in the page\n",
      "Title: Graph Anomaly Detection with Unsupervised GNNs\n",
      "\n",
      "\n",
      "\n",
      "Introduction:\n",
      "\n",
      "- Graph anomaly detection aims to identify unusual graphs in a database, with applications in many domains.\n",
      "\n",
      "- Prior work has limitations in handling complex graph properties and lacks end-to-end deep learning models.\n",
      "\n",
      "- This paper proposes an end-to-end graph anomaly detection model called GLAM based on graph neural networks (GNNs).\n",
      "\n",
      "\n",
      "\n",
      "Definition of GNNs:\n",
      "\n",
      "- GNNs operate on graph data to learn node representations by aggregating features from local graph neighborhoods.\n",
      "\n",
      "- Common GNN operations include graph convolution, attention, message passing etc. \n",
      "\n",
      "- After node embeddings are learned, GNNs can be used for node, edge, or graph-level prediction tasks.\n",
      "\n",
      "\n",
      "\n",
      "Method Used:\n",
      "\n",
      "- GLAM architecture has a GNN for node embeddings, followed by mean pooling and proposed MMD pooling for graph embeddings.\n",
      "\n",
      "- Trained end-to-end using a deep SVDD objective for anomaly detection.\n",
      "\n",
      "- Performs unsupervised model selection to pick the best model from candidates using HITS algorithm.\n",
      "\n",
      "\n",
      "\n",
      "Findings:\n",
      "\n",
      "- GLAM outperforms node-level and two-stage baselines on 15 graph datasets, showing the value of end-to-end learning.\n",
      "\n",
      "- MMD pooling complements mean pooling in detecting distribution anomalies.\n",
      "\n",
      "- Model selection consistently improves over average model performance.\n",
      "\n",
      "\n",
      "\n",
      "Key Points:\n",
      "\n",
      "- Proposed GLAM, an end-to-end graph anomaly detection model using GNNs.\n",
      "\n",
      "- Novel MMD pooling strategy to capture distribution anomalies.\n",
      "\n",
      "- Addressed the unsupervised hyperparameter tuning problem.\n",
      "\n",
      "- Demonstrated effectiveness on multiple benchmark datasets.\n",
      "\n",
      "\n",
      "\n",
      "Challenges:\n",
      "\n",
      "- Lack of ground truth anomalies and labels makes model evaluation difficult.\n",
      "\n",
      "- Large hyperparameter space of deep models poses tuning challenges.\n",
      "\n",
      "- Understanding what makes a graph anomaly and formalizing the intuition.\n",
      "\n",
      "\n",
      "\n",
      "Summary:\n",
      "\n",
      "The paper presents GLAM, a GNN-based end-to-end model for graph anomaly detection. A new MMD pooling strategy captures distribution anomalies. Experiments show GLAM outperforms baselines on 15 datasets. The unsupervised model selection component selects superior models without using labels.\n",
      "{'source': 'Files/Summary.docx'}\n"
     ]
    }
   ],
   "source": [
    "data = extract_text_from_document('Files/Summary.docx')\n",
    "print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[0].page_content)} characters in the page')\n",
    "print(data[0].page_content)\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d5b71",
   "metadata": {},
   "source": [
    "### Chunking Strategies and Splitting the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da33a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c803fe",
   "metadata": {},
   "source": [
    "### Calling the chunk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ef9d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "aDepartment of Computer Science and Technology, Tsinghua University, Beijing, China\n",
      "bSchool of Computer Science, Beijing University of Posts and Telecommunications, China\n",
      "cTencent Incorporation, Shenzhen, China\n",
      "ARTICLE INFO\n",
      "Keywords:\n"
     ]
    }
   ],
   "source": [
    "my_chunks = split_text_into_chunks(data)\n",
    "print(len(chunks))\n",
    "print(chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a2ad8",
   "metadata": {},
   "source": [
    "### Calculating the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c49d153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_display_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD:{total_tokens / 1000 * 0.0004:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4572475",
   "metadata": {},
   "source": [
    "### Load or create embedding index function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a66ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_embeddings_index(index_name,chunks):\n",
    "    import pinecone\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "    \n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings...', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Done')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end = '')\n",
    "        pinecone.create_index(index_name, dimension=1536, metric='cosine')\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name = index_name)\n",
    "        print('Done')\n",
    "        \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440cb67d",
   "metadata": {},
   "source": [
    "### Delete Pinecone Index Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b356511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    \n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "    if index_name == 'all':\n",
    "        indexes = pinecone.list_indexes()\n",
    "        print('Deleting all indexes ...')\n",
    "        for index in indexes:\n",
    "            pinecone.delete_index(index)\n",
    "        print('Done')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pinecone.delete_index(index_name)\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a22582",
   "metadata": {},
   "source": [
    "### Testing the Load or create embedding index function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa59e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Files/GNNs.pdf\n",
      "You have 25 pages in your data\n",
      "There are 6419 characters in the page\n"
     ]
    }
   ],
   "source": [
    "data = extract_text_from_document('Files/GNNs.pdf')\n",
    "print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[5].page_content)} characters in the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a7e39277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "my_chunks = chunk_data(data)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e45de78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 45310\n",
      "Embedding Cost in USD:0.018124\n"
     ]
    }
   ],
   "source": [
    "calculate_and_display_embedding_cost(my_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bcbcd192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "drop_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5aabb508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index gnnsdocument and embeddings ...Done\n"
     ]
    }
   ],
   "source": [
    "index_name='gnnsdocument'\n",
    "vector_store = load_or_create_embeddings_index(index_name, my_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c2d3f",
   "metadata": {},
   "source": [
    "### Creating the Function for Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71f78e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_from_vector_store(vector_store, question):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-4', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    answer = chain.invoke(question)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd8e1e",
   "metadata": {},
   "source": [
    "### Sending questions continuously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d74979c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type Quit or Exit to quit.\n",
      "Question #1:What is this document about?\n",
      "\n",
      "Answer: The document provides information about various research papers. One paper deals with question answering by reasoning across documents with graph convolutional networks, another paper talks about multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs. The document also states that there are no known competing financial interests or personal relationships that could affect the work and that the work is supported by the National Key Research and Development.\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Question #2:exit\n",
      "Exiting the application.... bye bye!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print(\"Please type Quit or Exit to quit.\")\n",
    "while True:\n",
    "    question = input(f'Question #{i}:')\n",
    "    i = i + 1\n",
    "    if question.lower() in ['quit', 'exit']:\n",
    "        print('Exiting the application.... bye bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    \n",
    "    answer = generate_answer_from_vector_store(vector_store, question)\n",
    "    answer_content = answer['result']\n",
    "    print(f'\\nAnswer: {answer_content}')\n",
    "    print(f'\\n {\"-\" * 100}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3fe8a",
   "metadata": {},
   "source": [
    "### Adding Memory (Chat History) to our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed871ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_conversation_with_context(vector_store, question, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "    \n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    result = crc.invoke({'question': question, 'chat_history': chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    \n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b34172",
   "metadata": {},
   "source": [
    "### Testing the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7c63434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN stands for Graph Neural Network. It is a type of neural network that is designed to handle and process structured data in the form of graphs. GNNs use a message passing mechanism between the nodes of a graph to capture dependencies and relationships within the data. They have been widely used in various applications, including natural language processing, computer vision, and recommendation systems, to incorporate graph-based information and improve the performance of tasks.\n",
      "[('What is GNN?', 'GNN stands for Graph Neural Network. It is a type of neural network that is designed to handle and process structured data in the form of graphs. GNNs use a message passing mechanism between the nodes of a graph to capture dependencies and relationships within the data. They have been widely used in various applications, including natural language processing, computer vision, and recommendation systems, to incorporate graph-based information and improve the performance of tasks.')]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"What is GNN?\"\n",
    "result, chat_history = conduct_conversation_with_context(vector_store, question, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "456ed824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is titled \"Question answering by reasoning across documents with graph convolutional networks\" and it was written by De Cao, N., Aziz, W., and Titov, I. The document focuses on the use of graph convolutional networks for question-answering tasks that require reasoning across multiple documents. It presents a model that can effectively reason over multiple documents by representing them as graphs and utilizing graph convolutional networks.\n",
      "[('What is this document about?', 'The document is titled \"Question answering by reasoning across documents with graph convolutional networks\" and it was written by De Cao, N., Aziz, W., and Titov, I. The document focuses on the use of graph convolutional networks for question-answering tasks that require reasoning across multiple documents. It presents a model that can effectively reason over multiple documents by representing them as graphs and utilizing graph convolutional networks.')]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"What is this document about?\"\n",
    "result, chat_history = conduct_conversation_with_context(vector_store, question, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2030ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text describes how the applications are categorized into structural and non-structural scenarios. It also mentions that for each scenario, several major applications and their corresponding methods are presented. However, it does not provide specific details on how the methods are discussed.\n",
      "[('How it discusses the method?', 'The text describes how the applications are categorized into structural and non-structural scenarios. It also mentions that for each scenario, several major applications and their corresponding methods are presented. However, it does not provide specific details on how the methods are discussed.')]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"How it discusses the method?\"\n",
    "result, chat_history = conduct_conversation_with_context(vector_store, question, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbb29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca632036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0fffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1912f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b53cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd1fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c2a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
